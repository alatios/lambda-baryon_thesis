%% Problema: deve essere grassetto nella ToC e nel titolo del capitolo, ma non grassetto nell'header.
\chapter{Signal event selection}
\label{cap:event_selection}
@todo: introduction

\section{Prefiltering}
\label{sec:prefilter}
\textit{Prefilters}, also refered to as \textit{preliminary selections} or simply \textit{pre-selections}, are the foundation of the signal selection process.
The main objective of this step is to improve the signal-to-background ratio and reduce the computational workload to analyze data with cuts on kinematic variables.

\begin{table}[t]
	\begin{center}
	\begin{tabular}{@{}llll@{}}
		\toprule
		Variable & Unit & Minimum & Maximum \\
		\midrule
		$p(p)$ 						& MeV/$c$ 	& 2\,000	& 500\,000 \\
		$p_T(p)$ 					& MeV/$c$ 	& 400		& -- \\
		$p(\pi^-)$ 					& MeV/$c$ 	& 10\,000	& 500\,000 \\
		$z_\Lambda^\text{vtx}$		& mm		& 5\,500	& 8\,500 \\
		$p_T(\Lambda^0)$ 			& MeV/$c$ 	& 450		& -- \\
		$m(p\pi^-)$	(Vertex Fitter)	& MeV/$c^2$	& 600		& 1\,500 \\
		$m(p\pi^-)$	(combined)		& MeV/$c^2$	& --		& 2\,000 \\
		$m(p\pi^-)$	(measured)		& MeV/$c^2$	& --		& 1\,500 \\
		$\cos\xi_p (\Lambda^0)$		& --		& 0.9999	& -- \\
		$\Delta \chi^2_\text{PV} (\Lambda^0)$
									& -- 		& --		& 200 \\
		$\chi^2_\text{dist} (\Lambda^0)$
									& --		& --		& $2\times{10}^{7}$ \\
		$\chi^2_\text{vtx} (\Lambda^0)$
									& --		& --		& 750 \\
		$\lvert m(\mu^+ \mu^-) - m_\text{PDG} (J/\psi) \rvert$
									& MeV/$c^2$ & --		& 90 \\
		$m(J/\psi~\Lambda^0)$ (combined)
									& MeV/$c^2$	& 4\,700		& -- \\
		$m(J/\psi~\Lambda^0)$ (Vertex Fitter)
									& MeV/$c^2$	& --		& 8\,500 \\
		$\lvert \cos\xi_p (\Lambda^0_b) \rvert$
									& --		& 0.99		& -- \\
		$\Delta \chi^2_\text{PV} (\Lambda^0_b)$
									& -- 		& --		& 1\,750 \\
		$\chi^2_\text{vtx} (\Lambda^0_b)$
									& --		& --		& 150 \\
		\bottomrule
	\end{tabular}
	\end{center}
	\caption{Prefilter selection criteria applied to simulated \demonstratorshort signal and Run 2 data. The Vertex Fitter invariant mass is computed by the homonymous algorithm; the \textit{combined} invariant mass is computed from the 4-momenta of the daughter particles at the first measurement position, without track extrapolation; the \textit{measured} invariant mass is computed in the same way as the combined mass, but after extrapolation at the reconstructed decay vertex of the mother particle. Angle $\xi_p$ for a particle is computed between the line connecting its origin and decay vertices and the direction of its momentum. $\Delta \chi^2_\text{PV}$ is the increase of the primary vertex $\chi^2$ when the particle is included in the fit. $\chi^2_\text{dist}$ is the geometrical distance between the primary vertex and the particle decay vertex in $\chi^2$ units.}
	\label{tab:4:prefilters}
\end{table}

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/LEVz_left.pdf}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/LEVz_right.pdf}
		\caption{}
	\end{subfigure}
	\caption{Efficiency of the $z_\text{vtx}^\Lambda \geq z_\text{cut}^\text{left}$ \textit{(a)} and $z_\text{vtx}^\Lambda \leq z_\text{cut}^\text{right}$ \textit{(b)} prefilter selection criteria on \demonstratorshort simulated signal, as function of the respective thresholds. The \textit{dotted vertical lines} mark the chosen thresholds.}
	\label{fig:4:z_lambda_cuts}
\end{figure}

The applied prefilter criteria are listed in Table \ref{tab:4:prefilters}.
Efficiencies on signal have been estimated with studies on simulated \demonstratorshort events.
The most impactful selection is the one applied to $z_\text{vtx}^\Lambda$, the $z$ component of the \lambdadecay decay vertex;
the efficiencies for the left and right cuts as a function of the threshold are shown in Figure \ref{fig:4:z_lambda_cuts}.
Since I require $\Lambda^0$ to decay after the dipole magnet in order to observe spin precession, the [@todo] efficiency of the $z_\text{vtx}^\Lambda \geq \SI{5.5}{\meter}$ cut cannot be avoided.
Other selections have a much lower impact on signal, with efficiencies $\gtrsim 80\%$, resulting in a total prefilter efficiency of [@todo].

As detailed in Section \ref{sec:2:tracking}, a key aspect of my analysis is the employment of T tracks for the reconstruction of the \lz.
The low residual magnetic field for protons and pions produced far from the dipole magnet lowers momentum resolution for the associated tracks down to $20$--$30\%$.
Resolution can be improved up to $\approx 10\%$ by placing kinematic constraints on $p\pi^-$ and $\mu^+ \mu^-$ invariant masses, fixing them to the PDG values of $m(\Lambda^0)$ and $m(J/\psi)$ respectively (these will be henceforth referred to as \textit{mass constraints}).
This approach cannot be implemented in the leaf-by-leaf framework of the default Vertex Fitter algorithm for vertex reconstruction;
instead each event is refitted with the Decay Tree Fitter (DTF) algorithm in two configurations (single $J/\psi$ and double $J/\psi~\Lambda^0$ mass constraints (see also Section \ref{sec:3:dtf}).

A convergence requirement of the DTF algorithm with the $J/\psi~\Lambda^0$ mass constraints is therefore added to the prefilter selections.
The main drawback of this selection, a much steeper [@todo] efficiency on simulated signal events, is outweighed by the benefits of the improved momentum resolution in the determination of the angular distribution of \lambdadecay decay products.


\subsection{Reconstruction of \texorpdfstring{\lz}{Lambda} decay vertex in prefiltered events}
\label{sec:lambda_endvertex_bias}

The quality of the \lambdadecay vertex reconstruction affects many aspects of the $\Lambda^0$ electromagnetic dipole moments measurement:
on top of being fundamental to evaluate how much magnetic field the particle traversed (and thus the extent of spin precession), even the best momentum resolution for protons and pions is worthless if the particles are extrapolated at the wrong point of production.
Both $x_\text{vtx}^\Lambda$ and $y_\text{vtx}^\Lambda$ are fairly well reconstructed in prefiltered events, with resolution $\lesssim \SI{1}{\centi\meter}$ and no discernible bias.
This section will therefore focus on the reconstruction of $z_\text{vtx}^\Lambda$.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/Lambda_endvertex_z_true.pdf}
		\caption{}
		\label{fig:4:lz_vertex_true}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/Lambda_endvertex_z.pdf}
		\caption{}
		\label{fig:4:lz_vertex_reco}
	\end{subfigure}
	\caption{Distribution of true \textit{(a)} and reconstructed \textit{(b)} $z_\text{vtx}^\Lambda$ in simulated \demonstratorshort signal events, without (\textit{dark grey}) and with (\textit{light grey}) prefiltering.}
	\label{fig:4:lz_vertex_distributions}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[height=.2\textheight]{graphics/04-event_selection/Lambda_endvertex_z_vs_x_true.pdf}
		\caption{}
		\label{fig:4:lz_vertex_peaks_true}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[height=.2\textheight]{graphics/04-event_selection/Lambda_endvertex_z_vs_x.pdf}
		\caption{}
		\label{fig:4:lz_vertex_peaks_reco}
	\end{subfigure}
	\caption{Distribution of simulated \demonstratorshort signal events (prefilters applied) with $z_\Lambda^\text{VF} \geq \SI{7.0}{\meter}$, as function of true (\textit{left}) and reconstructed (\textit{right}) $x_\Lambda^\text{vtx}$ and $z_\text{vtx}^\Lambda$. This corresponds to a top view of true and reconstructed \lz decay vertices.}
	\label{fig:4:lz_vertex_peaks}
\end{figure}

Figures \ref{fig:4:lz_vertex_true} and \ref{fig:4:lz_vertex_reco} show the distributions of true and reconstructed $z_\text{vtx}^\Lambda$ respectively for simulated signal events.
The most prominent difference between the two is the presence of three peaks in the $[\SI{7.5}{\meter},\SI{8.0}{\meter}]$ region of the reconstructed distribution, being found both with and without prefilter selections. 
The significance of these structures can be inferred by plotting the events as function of  $z_\text{vtx}^\Lambda$ and $x_\text{vtx}^\Lambda$, corresponding to a bending plane perspective of the detector.
This is shown in Figure \ref{fig:4:lz_vertex_peaks_reco}, highlighting the fact that the peaks in $\Lambda^0$ decay vertices have a very precise geometrical location, absent when comparing the true $z_\text{vtx}^\Lambda$ and $x_\text{vtx}^\Lambda$ values for the same events (Figure \ref{fig:4:lz_vertex_peaks_true}).
The spatial distribution of the vertices bears a striking resemblance to the layout of a T tracking station (see Figure \ref{fig:2:t_station_top}) and $z$ coordinates are consistent with the nominal placement of IT and first OT plane of the T1 station \cite{Barbosa-Marinho:582793}.
While dedicated studies are required to gain more insight into the source of these structures, they are assumed to be of minor impact for the purposes of this thesis.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/Lambda_endvertex_bias_z.pdf}
		\caption{}
		\label{fig:4:lz_endvertex_bias_linear}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/Lambda_endvertex_bias_z_log.pdf}
		\caption{}
		\label{fig:4:lz_endvertex_bias_log}
	\end{subfigure}
	\caption{Distribution of $z_\text{vtx}^\Lambda$ bias for simulated \demonstratorshort events in linear \textit{(a)} and logarithmic \textit{(b)} scales, without (\textit{dark grey}) and with (\textit{light grey}) prefiltering.}
	\label{fig:4:lz_endvertex_bias}
\end{figure}

The differing shapes of true and reconstructed $z_\text{vtx}^\Lambda$ distributions from Figure \ref{fig:4:lz_vertex_distributions} are also evidence of bias effects in the \lambdadecay vertex reconstruction.
This is confirmed in Figure \ref{fig:4:lz_endvertex_bias_linear}, showing the distribution of $z_\text{vtx}^\Lambda$ bias for simulated signal events:
the shape is distinctly non-gaussian, with a long tail towards the positive end of the axis counterbalancing the expected $\approx 0$ peak.
The resulting median bias is $\approx \SI{44}{\centi\meter}$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.7\textwidth]{graphics/04-event_selection/horizontality_illustration_bw.png}
	\caption{Deptiction of three \lambdadecay configurations and the associated horizontality values. The horizontal planes in the top and bottom diagrams are aligned to the LHCb $xz$ plane, the vertical plane in the middle diagram to the $yz$ plane.}
	\label{fig:4:horizontality_explanation}
\end{figure}

The positive bias tail can be interpreted as a mistake the vertexing algorithm commits when confronted with a specific decay geometry.
When the \lambdadecay decay plane closely aligns with the $xz$ bending plane, the bending induced by the magnet can produce either \textit{opening} or \textit{closing} tracks (depicted in top and bottom diagrams respectively in Figure \ref{fig:4:horizontality_explanation}.
In the latter case the tracks will cross again at $z>z_\text{vtx}^\Lambda$;
if $y$ displacement is sufficiently small, the algorithms converges on this <<ghost>> vertex instead of the real one.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/Lambda_horizontality_bias.pdf}
		\caption{}
		\label{fig:4:horizontality_bias}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/lambda_endvertex_z_bias_vs_horizontality_bias.pdf}
		\caption{}
		\label{fig:4:lz_endvertex_bias_vs_horizontality_bias}
	\end{subfigure}
	\caption{\textit{(a)} Horizontality bias distribution for simulated \demonstratorshort events (prefilters applied), comparing results from Decay Tree Fitter algorithm with \jpsi and \lz mass constraints with true values. \textit{(b)} Distribution of $z_\text{vtx}^\Lambda$ bias for events with horizontality bias $<1$ (\textit{horizontal hatching}) and $\geq 1$ (\textit{diagonal hatching}).}
\end{figure}

To test out this hypothesis we define the \textit{horizontality} of a \lambdadecay event as follows:
\begin{equation}
h = \sign{\left(\Lambda^0_\text{PID}\right)} \sign{\left(B_y\right)}~\frac{a_y}{\lvert \vec{a} \rvert},
\label{eq:4:horizontality}
\end{equation}
where
\begin{equation}
\vec{a} \coloneqq \vec{p}_p \times \vec{p}_\pi 
\end{equation}
is the cross product of proton and pion momenta at production vertex, $\sign{\left(B_y\right)}$ is the dipole magnet polarity\footnote{The LHCb dipole magnet polarity is reversed roughly twice per month to allow for studies on decay asymmetries \cite{Vesterinen:1642153}. The $B_y > 0$ configuration is conventionally known as \textit{magnet up} polarity, $B_y < 0$ as \textit{magnet down}.}
and $\sign{\left(\Lambda^0_\text{PID}\right)}$ is the sign of the PDG Monte Carlo particle numbering scheme of the mother particle ($+1$ for $\Lambda^0$, $-1$ for $\bar{\Lambda}^0$) \cite{PDG}.

Decays with $h=\pm1$ lie exactly on the $xz$ bending plane, $h=-1$ events having closing $p\pi^-$/$\bar{p}\pi^+$ tracks and $h=+1$ events having opening tracks, while $h=0$ events lie on the $yz$ plane (see Figure \ref{fig:4:horizontality_explanation}).
A horizontality bias $\Delta h \coloneqq h_\text{reco} - h_\text{true} > 1$ thus becomes the signature of a <<ghost>> vertex \lambdadecay event.
As per Figure \ref{fig:4:horizontality_bias}, this issue affects $\approx 25\%$ of reconstructed \demonstratorshort events, most of those being $\Delta h \approx 2$ events (from $h=-1$ to $h=+1$), with almost no event with $\Delta h < -1$.

Isolating $\Delta h \geq 1$ events and studying their $z_\text{vtx}^\Lambda$ bias distributions (Figure \ref{fig:4:lz_endvertex_bias_vs_horizontality_bias}), it becomes clear that they are largely responsible for the high bias observed in Figure \ref{fig:4:lz_endvertex_bias_linear}.
Significant asymmetry effects are still visible in the $\Delta h < 1$ distribution, which is still skewed towards positive bias.
While not ideal, this is somewhat expected given that the Vertex Fitter algorithm scans for candidate vertices starting from the first measurement position (i.e. the T1--T3 stations) and moving upstream.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/bump_Lambda_true_endvertex_z_vs_x.pdf}
		\caption{}
		\label{fig:4:bump_true}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/bump_scatter_Lambda_endvertex_z_vs_x.pdf}
		\caption{}
		\label{fig:4:bump_reco}
	\end{subfigure}
	\caption{Distribution of simulated \demonstratorshort events (prefilters applied) with $z_\Lambda^\text{VF} - z_\Lambda^\text{true} \geq \SI{2.0}{\meter}$ as function of true \textit{(a)} and reconstructed \textit{(b)} $x_\Lambda^\text{vtx}$ and $z_\text{vtx}^\Lambda$. This corresponds to a top view of true and reconstructed \lz decay vertices.}
	\label{fig:4:bump}
\end{figure}

Most \demonstratorshort events, even those with ghost vertex reconstruction, still maintain a limited $\lesssim \SI{1.0}{\meter}$ bias on $z_\text{vtx}^\Lambda$.
A smaller substructure with $\geq \SI{2.0}{\meter}$ bias emerges when plotting the distribution in logarithmic scale, as in Figure \ref{fig:4:lz_endvertex_bias_log}.
Figure \ref{fig:4:bump_true} provides a top view of the $\Lambda^0$ decay vertices of these evemts, showing the distribution of true $z_\text{vtx}^\Lambda$ and $x_\text{vtx}^\Lambda$.
Most $\Lambda^0$ in high bias events decay in the earlier sections of the detector ($z<\SI{3.0}{\meter}$);
the high spatial concentration in specific regions of the $xz$ plane, such as the <<wings>> around $z\approx \SI{1.0}{\meter}$, as well as the consistency between the placement of these structures and the location of the different LHCb subdetectors (cf. Figure \ref{fig:2:lhcb_diagram}), suggest that they may be the result of interaction with the material.

No selection on reconstructed variables is possible to filter this class of events:
Figure \ref{fig:4:bump_reco} shows that the $\Lambda^0$ vertices are reconstructed in seemingly arbitrary positions.
Their impact on the overall performance on signal is nevertheless neglectable, since this events amount to roughly [@todo] of the total simulated sample.

\section{Physical background veto}
\label{sec:4:phys_bkg}
%Most physics analyses conducted with hadron colliders are up against two different sources of background with the same (or similar) final state as the searched signal:
%\begin{enumerate}
%	\item \textit{combinatorial} background denotes events where the particles are produced 
%	\item \textit{physical} background denotes events where 
%\end{enumerate}

The main source of physical background for the \demonstratorfull decay is the similar
\begin{equation}
	B^0 \rightarrow J/\psi~(\rightarrow \mu^+ \mu^-)~K_S^0~(\rightarrow \pi^+ \pi^-).
\end{equation}
The final states of the two decays only differ for a $p \leftrightarrow \pi^+$ change.
The $K_S^0$ meson also has a similar mean lifetime to the $\Lambda^0$, thus we expect a sizeable number of $K_S^0$ decaying after the dipole magnet.
To top it off, the $B^0$ mass ([@todo]) is very close to that of $\Lambda_b^0$ ([@todo]) \cite{PDG}, muddying the waters in invariant mass fits.

As discussed in Section \ref{sec:2:pid}, the LHCb detector employs the two RICH systems to identify and distinguish between protons, pions and kaons;
in the case of $\Lambda^0$ decaying after the dipole magnet RICH1 contributions is impossible, but information from RICH2 would still be available for the vast majority of the decays at hand.
However, this is where the experimental nature of physics analyses with T tracks becomes relevant once again:
due to technical issues in the implementation, RICH2 information for particle identification is unavailable for T tracks recorded during LHC Runs 1 and 2\footnote{A large effort by the Milan and Valencia LHCb research groups is underway at the time of writing to implement RICH2 information in T track trigger lines for Run 3.}, making \physbkgshort discrimination much more difficult.

\label{sec:B0_veto}
\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/phys_bkg_ks_comparison.pdf}
		\caption{}
		\label{fig:4:phys_bkg_ks}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/phys_bkg_b0_comparison.pdf}
		\caption{}
		\label{fig:4:phys_bkg_b0}
	\end{subfigure}
	\caption{Comparison of simulated $m(\pi^+\pi^-)$ \textit{(a)} and $m(J/\psi~K^0_S)$ \textit{(b)} distributions: \demonstratorshort events with $p\rightarrow \pi^+$ mass hypothesis are labeled by \textit{horizontal hatching}, \physbkgshort events by \textit{diagonal hatching}.}
	\label{fig:4:phys_bkg_distributions}
\end{figure}

To address this issue, events must pass a physical background selection based on the results of a Decay Tree Fitter refit with $p\rightarrow \pi^+$ mass hypothesis and kinematic constraints on the $J/\psi$ and $K_S^0$ invariant masses.
The veto rejects an event if the DTF refit converges and $m(J/\psi~K_S^0)$ is too close in value to the $B^0$ PDG mass, i.e.
\begin{equation}
	\lvert
	m(J/\psi~K_S^0) - m_\text{PDG}(B^0)
	\rvert < m_\text{thres},
\end{equation}
with tunable threshold $m_\text{thres}$.

This approach was tested on simulated \demonstratorshort and \physbkgshort samples.
The variable $m(J/\psi~K_S^0)$ was chosen as opposed to $m(\pi^+\pi^-)$ due to the significant overlap between $m(\pi^+\pi^-)$ invariant mass distributions of actual $K_S^0 \rightarrow \pi^+ \pi^-$ events and \lambdadecay events with $p\rightarrow \pi^+$ mass hypothesis (Figure \ref{fig:4:phys_bkg_ks});
the $m(J/\psi~K_S^0)$ distributions are comparatively more separate (Figure \ref{fig:4:phys_bkg_b0}), allowing for a better selection on the variable.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[height=.2\textheight]{graphics/04-event_selection/phys_veto_efficiencies.pdf}
		\caption{}
		\label{fig:4:phys_bkg_veto_eff_thres}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[height=.2\textheight]{graphics/04-event_selection/phys_veto_sig_efficiencies_per_bin.pdf}
		\caption{}
		\label{fig:4:phys_bkg_veto_eff_mass}
	\end{subfigure}
	\caption[Efficiency of the physical background veto as a function of the invariant mass discrepancy threshold and of $J/\psi~\Lambda^0$ invariant mass bins.]{\textit{(a)} Efficiency of physical background veto as a function of the invariant mass discrepancy threshold on simulated signal (\demonstratorshort, solid) and background (\physbkgshort with proton mass hypothesis, dashed) events. Chosen threshold marked by dotted line. \textit{(b)} Efficiency of the veto on different $m(J/\psi~\Lambda^0)$ bins for \demonstratorshort signal events.}
	\label{fig:4:phys_bkg_veto_eff}
\end{figure}

Figure \ref{fig:4:phys_bkg_veto_eff_thres} shows the veto efficiency on \demonstratorshort signal and \physbkgshort physical background as function of the threshold $m_\text{thres}$.
The chosen value of $m_\text{thres} = \SI{70}{\mev}$ retains [@todo] of signal while rejecting [@todo] of background;
signal efficiency as function of the $m(J/\psi~\Lambda^0)$ invariant mass is depicted in Figure \ref{fig:4:phys_bkg_veto_eff_mass}.


\section{HBDT classifier}
\label{sec:HBDT}
Even after prefilter selections, combinatorial background exceeds the signal by a factor [@todo: dalle efficienze].
This section details my work in training and testing a multivariate classifier to discriminate \demonstratorshort events, in order to lower the signal-to-background ratio to acceptable levels for physics analysis.
After taking into consideration a wide range of classifiers, a histogram-based gradient boosting classification tree (also referred to as \textit{histogram-based boosted decision tree}, HBDT for short) was chosen by virtue of its superior performance in studies on sample data \cite{Pessina:BSc:2020}.
Training, optimization and performance testing of the classifier were made with the Scikit-learn 0.24.2 package \cite{scikit-learn} for Python 3.6.8 \cite{10.5555/1593511}.

\begin{figure}
	\centering
	\includegraphics[height=.3\textheight]{graphics/04-event_selection/decision_tree.pdf}
	\caption{Diagram representation of a decision tree classifier.}
	\label{fig:4:decision_tree}
\end{figure}

The basic layout of a binary decision tree classifier is sketched in Figure \ref{fig:4:decision_tree}:
a sequence of decision nodes applies binary conditions according to available information on the individual event and eventually reaches a leaf node associated to the \textit{event score} $t$, in this case the probability that the event is a \demonstratorshort decay.
Performance of the decision tree can be enhanced with \textit{boosting} \cite{Yann:2013}, whereby the ultimate classifier is built as a weighted average of a large number of weaker trees;
the result is known as a boosted decision tree (BDT).
In the adaptive boosting (AdaBoost) implementation used in this thesis, each tree $T_k$ is trained on a reweighted data sample that prioritizes events misclassified by the $T_{k-1}$ tree.
The usage of a histogram-based BDT, arranging input samples into integer-valued bins, allows for much faster estimators when working with large data samples ($n \gtrsim {10}^4$).

%(gradient tree boosting with logistic loss function for binary classification)

\subsection{Training and testing data}
\label{sec:4:train_test_data}

\begin{figure}[t]
	\centering
	\includegraphics[width=.6\textwidth]{graphics/04-event_selection/sig_bkg_distribution_balance.pdf}
	\caption{Signal (\textit{horizontal hatching}) and background (\textit{diagonal hatching}) data samples used for training the HBDT classifier.
	%Test samples are taken from the same pool in 1:9 ratio.
	}
	\label{fig:4:HBDT_training_data}
\end{figure}

Supervised training of a classifier requires signal and background datasets as input to learn to distinguish between the two classes.
For signal I used simulated \demonstratorshort events, while for the background I sampled side bands on the left ($\left[\SI{4870.2}{\mev \per c^2}, \SI{5020.2}{\mev \per c^2}\right]$) and right ($\left[\SI{6220.2}{\mev \per c^2}, \SI{6520.2}{\mev \per c^2}\right]$) of the expected $\Lambda_b^0$ peak  in the Run 2 data $m(J/\psi~\Lambda)$ distribution (see Figure \ref{fig:4:HBDT_training_data}).

I considered two options for the signal-to-background ratio:
\begin{itemize}
	\item \textit{balanced} training dataset: $\approx 145\,000$ events, evenly split between signal and background;
	\item \textit{unbalanced} training dataset: $\approx 73\,000$ signal events, $\approx 3.6$ million background events.
\end{itemize}
The standard approach to machine learning calls for a roughly balanced dataset to train the classifier on signal and background alike.
Given the disproportionate signal-to-background ratio in the case at hand, however, I also explored the unbalanced approach, aiming for improved background rejection at the price of (reasonably) subpar signal identification.
Classifiers trained with balanced or unbalanced datasets showed very similar performances after score threshold optimization (see Section \ref{sec:4:threshold_optimization}), the main difference being that balanced HBDTs require harsher score selection criteria;
since no reason surfaced to favour either approach, I opted to follow the more conventional path and empolyed a balanced training dataset.
Testing data for performance evaluation was sampled from the same pool as training data in a 1:9 ratio.

The following kinematic variables (also known as \textit{features}) were used for signal discrimination:
transverse and longitudinal momenta of $p$, $\pi^-$ ($J/\psi$ and $\Lambda^0$ mass constraints) and $J/\psi$ (no mass constraints);
coordinates of the \lambdadecay vertex position;
$\Lambda_b^0$ and $\Lambda^0$  pointing angles $\xi_p$, i.e. the angle between the particle line of flight and its momentum;
$\tilde{\chi}^2_\text{vtx}$ of the \demonstratorshort and \lambdadecay vertices;
the increase $\Delta \chi^2_\text{PV}$ of the primary vertex $\chi^2$ when a particle is included in the fit, for $\Lambda_b^0$ and $\Lambda^0$;
the distances $\chi^2_\text{dist}$ between $\Lambda_b^0$ and $\Lambda^0$ decay vertices and the primary vertex in $\chi^2$ units.


\subsection{Hyperparameter optimization}
\label{sec:4:hyperoptimization}

The supervised training process for a classifiers tunes a set of parameters to minimize a given loss function; in the HBDT case, for instance, decision node and tree weights are trained according to the output of a logistic loss function.
In contrast, \textit{hyperparameters} are parameters governing the learning process itself and thus require separate optimization.

I selected three hyperparamters for optimization:
\begin{itemize}
	\item learning rate $L \in [0.003, 0.006, 0.010, 0.015]$, the shrinkage factor for the contribution of each tree in the boosting process.
	\item maximum number of boosting iterations $n_\text{max}^\text{iter} \in [1\,500, 2\,500, 5\,000]$;
	\item maximum number of leaves for each tree $n_\text{max}^\text{leaves} \in [100, 200, 400, 800]$.
\end{itemize}
Other hyperparameters, such as the maximum number of bins and the minimum number of samples per leaf, were left with the default values of the Scikit-learn implementation.

I performed an exhaustive grid scan of the aforementioned hyperparameter values.
Each classifier was evaluated on training data using a stratified k-folds cross validation technique: the training sample is split in $k=5$ subsamples, the classifier is trained in rotation on $k-1$ samples and tested with the $k$-th, and the $k$ results are averaged together.
For a given score threshold $i$, we define \textit{precision} $P_i$ and \textit{recall} $R_i$ in terms of the true/false positive/negative rates: precision 
\begin{equation}
	P_i = \frac{n_\text{TP}}{n_\text{TP} + n_\text{FP}}
	\label{eq:4:precision}
\end{equation}
scores how often the classifier mistakes background for signal, while recall
\begin{equation}
	R_i = \frac{n_\text{TP}}{n_\text{TP} + n_\text{FN}}
	\label{eq:4:recall}
\end{equation}
scores how well the classifier is able to recognize the signal.
Performance of cross-validated classifiers on training and testing sub-samples\footnote{Still working within a cross-validation mindset, here \textit{testing data} is used to refer to the $k$-th fold in the cross-validation. No actual testing data as defined in Section \ref{sec:4:train_test_data} was used for the hyperoptimization step.} was evaluated using the \textit{average precision} figure of merit
\begin{equation}
	\text{AP} = \sum_{i=2}^{n} (R_i - R_{i-1})P_i
	\label{eq:4:average_prec}
\end{equation}
for a set of $n$ thresholds.

When considering the choice of the best classifier, I aimed to strike a balance between the AP score on testing data, acting as a raw performance grading of the classifier, and the difference in AP score between training and testing samples, which was necessary to prevent overtraining.
The HBDT classifier used for the remainder of this thesis, with $L=0.003$, $n_\text{max}^\text{iter}=2500$, and $n_\text{max}^\text{leaves}=100$, is the result of this selection process.

\subsection{Performance test}

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/confmatrix_train.pdf}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/confmatrix_test.pdf}
		\caption{}
	\end{subfigure}
	\caption{Confusion matrices visualizing the performance of the HBDT classifier on training \textit{(a)} and testing \textit{(b)} data samples. Percentages and chromatic scale are normalized to the true event classification: for instance, the top left and top right quadrants of a matrix represent the fraction of true background events reconstructed as background or signal, respectively. Binary classification uses an illustrative response threshold $t_\text{thres} = 0.5$.}
	\label{fig:4:confusion_matrix}
\end{figure}

Performance of the trained HBDT optimized as per Section \ref{sec:4:hyperoptimization} was evaluated on the smaller test sample described in Section \ref{sec:4:train_test_data}.
Figure \ref{fig:4:confusion_matrix} shows the confusion matrices for training and test data with a score threshold of $t_\text{thres}=0.5$, meaning events with $t>t_\text{thres}$ are classified as signal.
The matrices summarize the true/false positive/negative rates for the classifier, highlighting the expected slight dip in accuracy when moving from training to test sample.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/sig_train_vs_test.pdf}
		\caption{}
		\label{fig:4:sig_train_vs_test}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/bkg_train_vs_test.pdf}
		\caption{}
		\label{fig:4:bkg_train_vs_test}
	\end{subfigure}
	\caption{Response distribution of the HBDT classifier on signal \textit{(a)} and background \textit{(b)} events. The training sample is represented by \textit{horizontal hatching}, the test sample by \textit{diagonal hatching}.}
	\label{fig:4:train_vs_test}
\end{figure}

Comparing the signal and background score distributions (Figures \ref{fig:4:sig_train_vs_test} and \ref{fig:4:bkg_train_vs_test} respectively) for training and test data samples, we observe a light discrepancy at the end tails.
Such a split in favour of performance on training data is usually an early indicator of ovetraining.
I investigated this possibility by conducting a two-sample Kolmogorov-Smirnov (K-S) test \cite{10.2307/2280095}, which computes the probability $p$ that two samples are drawn from the same probability distribution.
Conventionally, the K-S is considered passed if $p>0.05$;
this was true for both signal ($p_\text{sig} \approx 0.30$) and background ($p_\text{bkg} \approx 0.07$), confirming that the classifier is not significantly ovetrained.

\begin{figure}
	\centering
		\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/roc.pdf}
		\caption{}
		\label{fig:4:hbdt_roc_curve}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/prec_recall.pdf}
		\caption{}
		\label{fig:4:hbdt_precall_curve}
	\end{subfigure}
	\caption{Receiving operating characteristic (ROC) curve \textit{(a)} and precision-recall curve \textit{(b)} for the HBDT classifier on training (\textit{solid}) and test (\textit{dashed}) samples. The legend in the ROC plot includes the area-under-curve (AUC) score.}
	\label{fig:4:hbdt_performance_curves}
\end{figure}

I assessed the classifier performance at different thresholds using the receiving operating characteristic (ROC) and precision-recall curves.
The ROC curve, depicted in Figure \ref{fig:4:hbdt_roc_curve}, plots the true positive versus false positive rates for a set of thresholds left as hidden variables:
a random classifier would bisect the plot plane, while a perfect classifier would adhere to the axes and cross the $(0,1)$ point.
The integral below the curve is itself a figure of merit, the area-under-curve (AUC), with better classifiers scoring values closer to unity.
The precision-recall curve follows a similar principle to the ROC curve, plotting precision \eqref{eq:4:precision} and recall \eqref{eq:4:recall} for different thresholds;
in this case, a perfect classifier would cross the $(1,1)$ point.
In both instances, the classifier shows great performance and high consistency between training and test samples.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{graphics/04-event_selection/importances.pdf}
	\caption{Decrease in the HBDT AP score when permutating the values of individual features in training (\textit{vertical hatching}) and testing (\textit{diagonal hatching}) data samples. See the legend of Table \ref{tab:4:prefilters} for details on the feature labels.}
	\label{fig:4:hbdt_importances}
\end{figure}

Finally, I evaluated the importance of selected features in signal discrimination.
This was achieved by permutating the values of each feature at a time and evaluating the average decrease in the baseline AP score of the classifier over ten of such permutations.
Results are shown in Figure \ref{fig:4:hbdt_importances} for training and testing samples:
in both cases the most discriminating features, causing the largest drops in AP, are the transverse momentum of the proton, the pointing angle of the $\Lambda_b^0$ and the increase in the primary vertex $\chi^2$ when including the $\Lambda_b^0$ in the fit.

\subsection{Threshold optimization}
\label{sec:4:threshold_optimization}

The final step in the classifier tuning process was the optimization of the threshold to maximize signal significance
\begin{equation}
\frac{s}{\sqrt{s+b}},
\label{eq:4:signal_significance}
\end{equation}
with $s$ and $b$ being the signal and background event counts after the HBDT filter.
A simple way to do it would be to fit the $J/\psi~\Lambda^0$ invariant mass distribution of filtered data with signal and background functions and extract the corresponding rates via integration;
however, this has the undesired side effect of heightening the risk of biasing the selection towards our specific data sample.
%A different, more general approach was devised instead.

A different, more general approach was devised to prevent this.
I first identified a <<soft>> score threshold $t_\text{thres} = 0.7$, just high enough for the filtered Run 2 data $m(J/\psi~\Lambda^0)$ distribution (using values from Decay Tree Fitter with mass constraints on both particles) to show the $\Lambda_b^0$ resonance peak.
I fit the simulated signal $m(J/\psi~\Lambda^0)$ distribution (using values from Decay Tree Fitter with mass constraints on both particles) with a double-tailed asymmetric Crystal Ball function.
The standard, single-tailed Crystal Ball probability density function is parameterized as
\begin{equation}
	f_\text{CB}(m; \mu, \sigma, \alpha, n) =
	\begin{cases}
		\exp\left[ -\frac{1}{2} {\left(\frac{m-\mu}{\sigma}\right)}^2 \right],
		& \text{if } \frac{m-\mu}{\sigma} \geq -\alpha \\
		A \cdot {\left(B - \frac{m-\mu}{\sigma}\right)}^{-n},
		& \text{if } \frac{m-\mu}{\sigma} < -\alpha
	\end{cases},
	\label{eq:4:crystal_ball}
\end{equation}
with
\begin{equation}
	A = {\left(\frac{n}{|\alpha|}\right)}^n~e^{-\frac{\alpha^2}{2}}
\end{equation}
and
\begin{equation}
	B = \frac{n}{|\alpha|} - |\alpha|.
\end{equation}
Its shape is thus a Gaussian core with a power-law tail on the lower end when $m$ outdistances $\mu$ by $\alpha\sigma$.
The double-tailed Crystal Ball expands on this concepts by having a Gaussian core with two asymmetric long tails, governed by different power laws:
\begin{equation}
	f_\text{sig}
	\left(m; S, \mu, \sigma, \alpha_1, n_1, \alpha_2, n_2 \right) =
	N
	\begin{cases}
	f_\text{CB}(m; \mu, \sigma, \alpha_1, n_1)
	& \text{if } m \leq \mu \\
	f_\text{CB}(2\mu -m; \mu, \sigma, \alpha_2, n_2)
	& \text{if } m > \mu \\
	\end{cases},
\end{equation}
with normalization
\begin{equation}
	N = \frac{S}{\sigma(C_1 + C_2 + D)}
\end{equation}
itself depending on
\begin{equation}
	C_1 = \frac{n_1}{\alpha_1 (n_1-1)} e^{-\frac{\alpha^2_1}{2}},
\end{equation}
\begin{equation}
	C_2 = \frac{n_2}{\alpha_2 (n_2-1)} e^{-\frac{\alpha^2_2}{2}},
\end{equation}
and
\begin{equation}
	D = \sqrt{2\pi} \left(G_\text{cum} (\alpha_2) - G_\text{cum} (- \alpha_1)\right),
\end{equation}
where
\begin{equation}
G_\text{cum}(x) \coloneqq \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x dt e^{-\frac{t^2}{2}}
\end{equation}
is the Gaussian cumulative distribution function evaluated at $x$.

When fitting $m(J/\psi~\Lambda^0$ in Run 2 data, the function is the sum of two contributions:
signal is modeled with the same double-tailed Crystal Ball from \eqref{eq:4:crystal_ball}, with all parameters bar $S$ fixed to their Monte Carlo fit best values;
the combinatorial background is fit with a turn-on exponential
\begin{equation}
	f_\text{bkg} (m;C,m_0,a) = C \sqrt{m-m_0}~e^{-a(m-m_0)},
	\label{eq:4:turn_on_exp}
\end{equation}
with $m_0$ being the turn-on point.
As will be shown in Section \ref{sec:4:performance_data}, the outlined combination of $f_\text{sig}$ and $f_\text{bkg}$ provides an excellent description of Run 2 data, as long as sufficient filtering is applied for the $\Lambda_b^0$ peak to emerge.

This data fit provides preliminary $s_0$ and $b_0$ values, obtained by integrating signal and background fit functions in the $\pm 3\sigma$ region around $\mu$.
I used these values to estimate the signal (background) rate $s_i$ ($b_i$) for events passing a score threshold $t_i$ with associated efficiency $\varepsilon_i^s$ ($\varepsilon_i^b$):
\begin{subequations}
\begin{align}
	s_i &= s_0 \frac{\varepsilon_i^s}{\varepsilon_0^s}, \\
	b_i &= b_0 \frac{\varepsilon_i^b}{\varepsilon_0^b}.
\end{align}
\end{subequations}
Here $\varepsilon_0^s$ and $\varepsilon_0^b$ are the signal and background efficiencies associated to the soft threshold with $s_0$ and $b_0$ rates.
Efficiencies were computed on the test sample and correspond to true/false positive rates evaluated for the ROC curve (Figure \ref{fig:4:hbdt_roc_curve}).

\begin{figure}
	\centering
	\includegraphics[width=.6\textwidth]{graphics/04-event_selection/HBDT_signal_significance.pdf}
	\caption{Projected \demonstratorshort signal significance over background as a function of the HBDT response threshold used for selection.}
	\label{fig:4:signal_significance}
\end{figure}

Using the estimated $s_i$ and $b_i$, I calculated the predicted signal significance \eqref{eq:4:signal_significance} for each threshold $t_i$.
Figure \ref{fig:4:signal_significance} shows the results as a function of the score threshold.
The best performing threshold was found to be $t_\text{thres} = 0.985$, with signal significance $\approx 45$ and a signal-to-background ratio $\approx 1.5$.

\section{Performance on data}
\label{sec:4:performance_data}
Gli invariant mass fits, essenzialmente.

\begin{figure}[t]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/MC_lambdab_hard_fit.pdf}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{graphics/04-event_selection/data_lambdab_hard_fit.pdf}
		\caption{}
	\end{subfigure}
	\caption{Fitted $m(J/\psi~\Lambda^0)$ invariant mass distributions for simulated \demonstratorshort events \textit{(a)} and Run 2 data \textit{(b)} after all selection steps. Signal fit function is \textit{dashed}, background fit function in \textit{(b)} is \textit{dash-dotted}. The PDG value for the $\Lambda_b^0$ mass is marked by the \textit{dotted vertical line} \cite{PDG}. Fit pulls (data-fit discrepancy divided by uncertainty) are shown below the main plots.}
\end{figure}
